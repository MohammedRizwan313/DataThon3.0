{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d403b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import validators\n",
    "from googletrans import Translator\n",
    "from translate import Translator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "706a75dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Date</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text_of_Speech</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Running President/PM</th>\n",
       "      <th>Speech Link</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia</td>\n",
       "      <td>16/07/2021</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>Meeting of APEC Economic Leaders</td>\n",
       "      <td>Madam Chair,\\n\\nColleagues,\\n\\nFirst of all, ...</td>\n",
       "      <td>President</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>http://en.kremlin.ru/events/president/transcri...</td>\n",
       "      <td>OPTIMISM</td>\n",
       "      <td>DEVELOPMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russia</td>\n",
       "      <td>2021-09-05 00:00:00</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>Victory Parade on Red Square</td>\n",
       "      <td>Citizens of Russia,\\n\\nDear veterans,\\n\\nComra...</td>\n",
       "      <td>President</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>http://en.kremlin.ru/events/president/transcri...</td>\n",
       "      <td>JOY</td>\n",
       "      <td>NATIONALISM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russia</td>\n",
       "      <td>2021-08-04 00:00:00</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>Meeting on the results of implementing Preside...</td>\n",
       "      <td>Good afternoon, colleagues.\\n\\nLet’s start.\\n\\...</td>\n",
       "      <td>President</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>http://en.kremlin.ru/events/president/transcri...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>DEVELOPMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia</td>\n",
       "      <td>21-11-2020</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>G20 Summit</td>\n",
       "      <td>Colleagues,\\n\\nThe scope of problems humanity ...</td>\n",
       "      <td>President</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>http://en.kremlin.ru/events/president/transcri...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>DEVELOPMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russia</td>\n",
       "      <td>20-11-2020</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>Address to participants in Nuremberg Lessons f...</td>\n",
       "      <td>Colleagues, friends,\\n\\nFirst of all, I would ...</td>\n",
       "      <td>President</td>\n",
       "      <td>Vladimir Putin</td>\n",
       "      <td>http://en.kremlin.ru/events/president/transcri...</td>\n",
       "      <td>UPSET</td>\n",
       "      <td>EXTREMISM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country                 Date         Speaker  \\\n",
       "0  Russia           16/07/2021  Vladimir Putin   \n",
       "1  Russia  2021-09-05 00:00:00  Vladimir Putin   \n",
       "2  Russia  2021-08-04 00:00:00  Vladimir Putin   \n",
       "3  Russia           21-11-2020  Vladimir Putin   \n",
       "4  Russia           20-11-2020  Vladimir Putin   \n",
       "\n",
       "                                            Headline  \\\n",
       "0                   Meeting of APEC Economic Leaders   \n",
       "1                       Victory Parade on Red Square   \n",
       "2  Meeting on the results of implementing Preside...   \n",
       "3                                         G20 Summit   \n",
       "4  Address to participants in Nuremberg Lessons f...   \n",
       "\n",
       "                                      Text_of_Speech Designation  \\\n",
       "0   Madam Chair,\\n\\nColleagues,\\n\\nFirst of all, ...   President   \n",
       "1  Citizens of Russia,\\n\\nDear veterans,\\n\\nComra...   President   \n",
       "2  Good afternoon, colleagues.\\n\\nLet’s start.\\n\\...   President   \n",
       "3  Colleagues,\\n\\nThe scope of problems humanity ...   President   \n",
       "4  Colleagues, friends,\\n\\nFirst of all, I would ...   President   \n",
       "\n",
       "  Running President/PM                                        Speech Link  \\\n",
       "0       Vladimir Putin  http://en.kremlin.ru/events/president/transcri...   \n",
       "1       Vladimir Putin  http://en.kremlin.ru/events/president/transcri...   \n",
       "2       Vladimir Putin  http://en.kremlin.ru/events/president/transcri...   \n",
       "3       Vladimir Putin  http://en.kremlin.ru/events/president/transcri...   \n",
       "4       Vladimir Putin  http://en.kremlin.ru/events/president/transcri...   \n",
       "\n",
       "    Emotion      Context  \n",
       "0  OPTIMISM  DEVELOPMENT  \n",
       "1       JOY  NATIONALISM  \n",
       "2   NEUTRAL  DEVELOPMENT  \n",
       "3   NEUTRAL  DEVELOPMENT  \n",
       "4     UPSET    EXTREMISM  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Dataset - EMPOLITICON NLP and ML Based Approach for Context and Emotion Classification of Political Speeches From Transcripts.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7ce13b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[\"Text_of_Speech\"]\n",
    "y_train = data[\"Emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cf7f6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "null_value_index = x_train.loc[x_train.isnull()].index\n",
    "print(null_value_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "770e8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(null_value_index, axis=0,inplace = True)\n",
    "y_train.drop(null_value_index, axis=0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "09117257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "null_value_index = y_train.loc[y_train.isnull()].index\n",
    "print(null_value_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c5dbbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(null_value_index, axis=0,inplace = True)\n",
    "y_train.drop(null_value_index, axis=0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "394334ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "69cb46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "37bcebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_un(text):\n",
    "    te = \"\"\n",
    "    for i in text.split(\"\\n\"):\n",
    "        if len(i) > 0:\n",
    "            te = te + i \n",
    "    return te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cc5f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_un1(text):\n",
    "    te = \"\"\n",
    "    for i in text.split(\"\\t\"):\n",
    "        if len(i) > 0:\n",
    "            te = te + i \n",
    "    return te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5f63463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2f6a9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.apply(remove_un)\n",
    "x_train = x_train.apply(remove_un1)\n",
    "x_train = x_train.apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "25b8305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "430850fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_stop_words(text):\n",
    "    data = nltk.word_tokenize(text)\n",
    "    te = \"\"\n",
    "    for i in data:\n",
    "        if i.strip().lower() not in stop_words:\n",
    "            new = lem.lemmatize(i)\n",
    "            te = te + new + \" \"\n",
    "    return te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "de430ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.apply(removing_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "427db70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size = 0.25,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a4282d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = speeches = x_train.tolist()\n",
    "bag_of_words = [nltk.word_tokenize(speech.lower()) for speech in x_train_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4bdb4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_list = speeches = x_test.tolist()\n",
    "bag_of_words2 = [nltk.word_tokenize(speech.lower()) for speech in x_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e8d7e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=bag_of_words, vector_size=100, window=5, min_count=1, sg=0)\n",
    "word2vec_model.train(bag_of_words, total_examples=len(bag_of_words), epochs=10)\n",
    "vector = word2vec_model.wv['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5721e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model2 = Word2Vec(sentences=bag_of_words2, vector_size=100, window=5, min_count=1, sg=0)\n",
    "word2vec_model.train(bag_of_words2, total_examples=len(bag_of_words2), epochs=10)\n",
    "vector = word2vec_model.wv['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9a46b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_vectors(sentence, model):\n",
    "    vector_sum = np.zeros(model.vector_size)\n",
    "    num_words = 0\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            vector_sum += model.wv[word]\n",
    "            num_words += 1\n",
    "    if num_words > 0:\n",
    "        return vector_sum / num_words\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "81fe4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word2vec = [aggregate_vectors(sentence, word2vec_model) for sentence in bag_of_words ]\n",
    "X_train_word2vec_matrix = np.vstack(X_train_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0e5dc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word2vec = [aggregate_vectors(sentence, word2vec_model2) for sentence in bag_of_words2]\n",
    "X_test_word2vec_matrix = np.vstack(X_train_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3490eb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_word2vec_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e68c466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_count_matrix=vectorizer.fit_transform(x_train)\n",
    "# X_test_count_matrix = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ed271",
   "metadata": {},
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "aaa7e2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8189054726368159\n"
     ]
    }
   ],
   "source": [
    "logic = LogisticRegression()\n",
    "logic.fit(X_train_count_matrix,y_train)\n",
    "print(logic.score(X_train_count_matrix,y_train))\n",
    "# y_pred = logic.predict(X_test_count_matrix)\n",
    "# print(\"accuracy = \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af681744",
   "metadata": {},
   "source": [
    "#### using the word to vector modeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "599d7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.2962226640159046\n"
     ]
    }
   ],
   "source": [
    "logic = LogisticRegression(max_iter=1000)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_word2vec_matrix)\n",
    "\n",
    "logic.fit(X_train_scaled ,y_train)\n",
    "# print(logic.score(X_train_count_matrix,y_train))\n",
    "y_pred = logic.predict(X_test_word2vec_matrix)\n",
    "print(\"accuracy = \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51af6e",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a73ff027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.48111332007952284\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(probability=True)\n",
    "svc.fit(X_train_count_matrix,y_train)\n",
    "# print(logic.score(X_train_count_matrix,y_train))\n",
    "y_pred = svc.predict(X_test_count_matrix)\n",
    "print(\"accuracy = \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ba253",
   "metadata": {},
   "source": [
    "#### decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "70c6731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.3518886679920477\n"
     ]
    }
   ],
   "source": [
    "dec = DecisionTreeClassifier()\n",
    "dec.fit(X_train_count_matrix,y_train)\n",
    "y_pred = dec.predict(X_test_count_matrix)\n",
    "print(\"accuracy = \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2310a",
   "metadata": {},
   "source": [
    "#### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "83a97a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.48906560636182905\n"
     ]
    }
   ],
   "source": [
    "ran = RandomForestClassifier()\n",
    "ran.fit(X_train_count_matrix,y_train)\n",
    "y_pred = ran.predict(X_test_count_matrix)\n",
    "print(\"accuracy = \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7c1f7",
   "metadata": {},
   "source": [
    "#### k nearest nighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "80cae61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.4234592445328032\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_count_matrix,y_train)\n",
    "y_pred = knn.predict(X_test_count_matrix)\n",
    "print(\"accuracy = \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ab0deaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = VotingClassifier(estimators=[('logic', LogisticRegression()), ('SVC',SVC(probability=True)),(\"ran\",RandomForestClassifier())], voting='soft').fit(X_train_count_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "95ac7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred4 = final.predict(X_test_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5247b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.48906560636182905\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy = \",accuracy_score(y_test,ypred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf569a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
